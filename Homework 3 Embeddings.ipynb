{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3: Embeddings\n",
    "\n",
    "Please fill out below items\n",
    "\n",
    "* `Your name`: ``\n",
    "* `Andrew ID`: ``\n",
    "\n",
    "### Classifying newswires: a multi-class classification example\n",
    "\n",
    "In this homework, you will continue to build on the classification model from homework 1 to classify Reuters newswires. Specifically you'll be using embeddings technique to try and enhance your model from the first homework. \n",
    " \n",
    "\n",
    "### The Reuters dataset\n",
    "\n",
    "We will be working with the _Reuters dataset_, a set of short newswires and their topics, published by Reuters in 1986. It's a very simple, widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each topic has at least 10 examples in the training set.\n",
    "\n",
    "Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Like with the IMDB dataset, the argument `num_words=15000` restricts the data to the 15,000 most frequently occurring words found in the data. Let's load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks: \n",
    "* You will be training your own embeddings for tokens in the data set. \n",
    "* You will be downloading gensim's embeddings and adopting it to be used in a classifier.\n",
    "To receive full credit you should have completed tasks in all the sections below: `[10 points] + [1 bonus point]`\n",
    "\n",
    "## __Deliverables__ \n",
    "* Jupyter notebook\n",
    "* HTML file of jupyter notebook `[Goto: File-->Download as--> HTML]`\n",
    "\n",
    "## __Specific Tasks__\n",
    "* ### __Section 1: Build `Custom Embeddings` `6 points`__\n",
    "    * Print the train and test data size\n",
    "    * Decode the first article and print it's raw text\n",
    "    * Prepare the data for modeling as required and create a `validation set`\n",
    "    * Build the model with an embedding layer, and fully connected dense layers \n",
    "        * Choose an embedding length size of 50\n",
    "        * At-least 2 dense layers (apart from output layer), use RELU activation\n",
    "        * Use appropriate output layer\n",
    "        * Use dropouts                \n",
    "        * Use >250 epochs\n",
    "    * Plot the `loss curves` and `accuracy curves` between train and validation set\n",
    "    * Comment on what you're observing and record your best validation set accuracy\n",
    "    * __BONUS TASK: Explore the learnt embedding +1 point__\n",
    "        * Extract the embedding weights, pick one token and find words similar to it.\n",
    "* ### __Section 2: Use `Glove embeddings` `4 points`__\n",
    "    * Use the glove embeddings and per token. \n",
    "    * Feed that as input to fully connected dense layers\n",
    "        * At-least 2 dense layers (apart from output layer), use RELU activation\n",
    "        * Use appropriate output layer\n",
    "        * Use dropouts        \n",
    "        * Use >250 epochs\n",
    "    * Plot the `loss curves` and `accuracy curves` between train and validation set\n",
    "    * Comment on what you're observing about the tradeoffs of using pretrained embeddings vs starting them from scratch. Record your best validation set accuracy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Begin Coding below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Edit me"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
